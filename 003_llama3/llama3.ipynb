{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJFYKowdyLOE"
      },
      "source": [
        "# 003_Llam3 실습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 패키지 설치하기\n",
        "   - accelerate 패키지: Hugging Face 에서 제공하는 패키지로 PyTorch 모델을 간략하고 효율적으로 처리할 수 있음.*이탤릭체 텍스트*\n"
      ],
      "metadata": {
        "id": "umOgfcNjzO9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "jcQIacDyzVN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 토큰 설정하기"
      ],
      "metadata": {
        "id": "eDR6vq5tEK2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = \"발급 받은 토큰을 입력 하기\"\n"
      ],
      "metadata": {
        "id": "50z2pmCREOJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx21-uOmyLOG"
      },
      "source": [
        "## 패키지 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq2AqpG_yLOH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nw10s03yLOI"
      },
      "source": [
        "## 토큰 및 모델 다운로드하기\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2oCZ0q2yLOI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM 응답 함수"
      ],
      "metadata": {
        "id": "9yUrDTv24AeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNjCX-PqyLOI"
      },
      "outputs": [],
      "source": [
        "def generate_response(system_message, user_message):\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_message},\n",
        "  ]\n",
        "\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "  terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=512,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9  )\n",
        "\n",
        "  response = outputs[0][input_ids.shape[-1]:]\n",
        "  return tokenizer.decode(response, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ulqU5CyLOJ"
      },
      "source": [
        "## LLM 응답 함수 테스트 하기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/003_llama3/NvidiaSmiMonitor.py\n",
        "from NvidiaSmiMonitor import NvidiaSmiMonitor\n",
        "import sys\n",
        "import time\n",
        "\n",
        "monitor = NvidiaSmiMonitor(interval=60)  # 2초마다 실행\n",
        "monitor.start()\n",
        "\n",
        "# 10초 동안 실행 후 종료\n",
        "#time.sleep(10)\n"
      ],
      "metadata": {
        "id": "5Y8in4HlOvO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P1H3zbfyLOJ"
      },
      "outputs": [],
      "source": [
        "response = generate_response(system_message=\"\",\n",
        "                              user_message=\"한국어로 대답해줘. 배화여대에 대해서 말해줘\")\n",
        "print(response)\n",
        "\n",
        "\n",
        "monitor.stop()\n",
        "monitor.join()\n",
        "print(\"monitor---------종료\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}